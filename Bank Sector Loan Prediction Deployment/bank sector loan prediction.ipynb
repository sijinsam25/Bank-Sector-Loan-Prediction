{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Sector Loan Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "import pandas_profiling as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training dataset in a dataframe using Pandas\n",
    "lp=pd.read_csv('../input/banking-sector-loan-prediction/train (1).csv')\n",
    "lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(lp).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intuitively insignificant variables are deleted.\n",
    "lp.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminating the na values from the dataset\n",
    "#lp=lp.dropna(subset=['MIS_Status','Name','City','Bank','BankState','State','DisbursementDate'])\n",
    "#lp.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Distribution of Catergorial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the values into 0 and 1 in MIS_Status\n",
    "lp['MIS_Status'].value_counts()\n",
    "lp['MIS_Status']=lp['MIS_Status'].map({'P I F':int('0'),'CHGOFF':int('1')})\n",
    "lp['MIS_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIS_Status_mode=lp['MIS_Status'].value_counts().index[0]\n",
    "MIS_Status_mode\n",
    "lp['MIS_Status'].fillna(MIS_Status_mode,inplace=True)\n",
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='MIS_Status',data=lp,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the values into 0 and 1 in RevLineCr\n",
    "lp['RevLineCr'].value_counts()\n",
    "lp['RevLineCr']=lp['RevLineCr'].map({'N':int('0'),'Y':int('1')})\n",
    "lp['RevLineCr'].value_counts()\n",
    "lp.info()\n",
    "RevLineCr_mode=lp['RevLineCr'].value_counts().index[0]\n",
    "RevLineCr_mode\n",
    "lp['RevLineCr'].fillna(RevLineCr_mode,inplace=True)\n",
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='RevLineCr',data=lp,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the values into 0 and 1 in LowDoc\n",
    "lp['LowDoc'].value_counts()\n",
    "lp['LowDoc']=lp['LowDoc'].str.replace('C','Nan')\n",
    "lp['LowDoc']=lp['LowDoc'].str.replace('1','Nan')\n",
    "lp['LowDoc'].value_counts()\n",
    "lp['LowDoc']=lp['LowDoc'].map({'N':int('0'),'Y':int('1')})\n",
    "lp['LowDoc'].value_counts()\n",
    "lp.info()\n",
    "lp['LowDoc'].value_counts()\n",
    "LowDoc_mode=lp['LowDoc'].value_counts().index[0]\n",
    "LowDoc_mode\n",
    "lp['LowDoc'].fillna(LowDoc_mode,inplace=True)\n",
    "lp.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='LowDoc',data=lp,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in GrAppv\n",
    "lp['GrAppv'].value_counts()\n",
    "lp['GrAppv']=lp['GrAppv'].str.replace('$',' ')\n",
    "lp['GrAppv']=lp['GrAppv'].str.replace(',','')\n",
    "lp['GrAppv']=lp['GrAppv'].astype(float)\n",
    "lp['GrAppv'].value_counts()\n",
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['GrAppv'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in SBA_Appv\n",
    "lp['SBA_Appv'].value_counts()\n",
    "lp['SBA_Appv']=lp['SBA_Appv'].str.replace('$',' ')\n",
    "lp['SBA_Appv']=lp['SBA_Appv'].str.replace(',','')\n",
    "lp['SBA_Appv']=lp['SBA_Appv'].astype(float)\n",
    "lp['SBA_Appv'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in SBA_Appv\n",
    "lp['ChgOffPrinGr'].value_counts()\n",
    "lp['ChgOffPrinGr']=lp['ChgOffPrinGr'].str.replace('$',' ')\n",
    "lp['ChgOffPrinGr']=lp['ChgOffPrinGr'].str.replace(',','')\n",
    "lp['ChgOffPrinGr']=lp['ChgOffPrinGr'].astype(float)\n",
    "lp['ChgOffPrinGr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in BalanceGross\n",
    "lp['BalanceGross'].value_counts()\n",
    "lp['BalanceGross']=lp['BalanceGross'].str.replace('$',' ')\n",
    "lp['BalanceGross']=lp['BalanceGross'].str.replace(',','')\n",
    "lp['BalanceGross']=lp['BalanceGross'].astype(float)\n",
    "lp['BalanceGross'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in DisbursementGross\n",
    "lp['DisbursementGross'].value_counts()\n",
    "lp['DisbursementGross']=lp['DisbursementGross'].str.replace('$',' ')\n",
    "lp['DisbursementGross']=lp['DisbursementGross'].str.replace(',','')\n",
    "lp['DisbursementGross']=lp['DisbursementGross'].astype(float)\n",
    "lp['DisbursementGross'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DG_mode=lp['DisbursementDate'].value_counts().index[0]\n",
    "DG_mode\n",
    "lp['DisbursementDate'].fillna(DG_mode,inplace=True)\n",
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['DisbursementGross'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(lp).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az=pp.ProfileReport(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "lp.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the Highly correlated Variable \n",
    "GrAppv,SBA_Appv,DisbursementGross are highly correlated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = lp.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "lp.drop(lp[to_drop], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert ApprovalDate the particular datetime value\n",
    "#lp['ApprovalDate'].value_counts()\n",
    "#lp['ApprovalDate']=pd.to_datetime(lp['ApprovalDate'])\n",
    "#lp['ApprovalDate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert ChgOffDate the particular datetime value\n",
    "#lp['ChgOffDate'].value_counts()\n",
    "#lp['ChgOffDate']=pd.to_datetime(lp['ChgOffDate'])\n",
    "#lp['ChgOffDate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert DisbursementGross the particular datetime value\n",
    "#lp['DisbursementDate'].value_counts()\n",
    "#lp['DisbursementDate']=pd.to_datetime(lp['DisbursementDate'])\n",
    "#lp['DisbursementDate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers of NoEmp,RetainedJob,CreatedJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier of NoEmp\n",
    "#it is highly Skewed.removing the outliers and imputating it.\n",
    "\n",
    "## Perform all the steps of IQR\n",
    "sorted(lp['NoEmp'])\n",
    "quantile1, quantile3= np.percentile(lp['NoEmp'],[25,75])\n",
    "print(quantile1,quantile3)\n",
    "## Find the IQR\n",
    "iqr_value=quantile3-quantile1\n",
    "print(iqr_value)\n",
    "## Find the lower bound value and the higher bound value\n",
    "lower_bound_val = quantile1 -(1.5 * iqr_value) \n",
    "upper_bound_val = quantile3 +(1.5 * iqr_value)\n",
    "print(lower_bound_val,upper_bound_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['NoEmp'] = np.where(lp['NoEmp'] <-7.0, -7.0,lp['NoEmp'])\n",
    "lp['NoEmp'] = np.where(lp['NoEmp'] >17.0, 17.0,lp['NoEmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lp['NoEmp'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['NoEmp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['NoEmp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier of RetainedJob\n",
    "#it is highly Skewed.removing the outliers and imputating it.\n",
    "## Perform all the steps of IQR\n",
    "sorted(lp['RetainedJob'])\n",
    "quantile1, quantile3= np.percentile(lp['RetainedJob'],[25,75])\n",
    "print(quantile1,quantile3)\n",
    "## Find the IQR\n",
    "iqr_value=quantile3-quantile1\n",
    "print(iqr_value)\n",
    "## Find the lower bound value and the higher bound value\n",
    "lower_bound_val = quantile1 -(1.5 * iqr_value) \n",
    "upper_bound_val = quantile3 +(1.5 * iqr_value)\n",
    "print(lower_bound_val,upper_bound_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['RetainedJob'] = np.where(lp['RetainedJob'] <-6.0, -6.0,lp['RetainedJob'])\n",
    "lp['RetainedJob'] = np.where(lp['RetainedJob'] >10.0, 10.0,lp['RetainedJob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lp['RetainedJob'].skew())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['RetainedJob'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['RetainedJob'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RetainedJob variable having zeroes. using imputation replacing the zero value\n",
    "\n",
    "imp = SimpleImputer(missing_values=0,strategy = 'most_frequent')\n",
    "\n",
    "mode_imp = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "dfseries = lp.RetainedJob\n",
    "\n",
    "dfseries.values\n",
    "dfseries.values.reshape(-1,1).shape\n",
    "reshdf = dfseries.values.reshape(-1,1)\n",
    "\n",
    "reshdf.shape\n",
    "imp.fit_transform(reshdf)\n",
    "lp.RetainedJob = imp.fit_transform(reshdf)\n",
    "lp['RetainedJob'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lp['RetainedJob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier of RetainedJob\n",
    "#it is highly Skewed.removing the outliers and imputating it.\n",
    "## Perform all the steps of IQR\n",
    "sorted(lp['CreateJob'])\n",
    "quantile1, quantile3= np.percentile(lp['CreateJob'],[25,75])\n",
    "print(quantile1,quantile3)\n",
    "## Find the IQR\n",
    "iqr_value=quantile3-quantile1\n",
    "print(iqr_value)\n",
    "## Find the lower bound value and the higher bound value\n",
    "lower_bound_val = quantile1 -(1.5 * iqr_value) \n",
    "upper_bound_val = quantile3 +(1.5 * iqr_value)\n",
    "print(lower_bound_val,upper_bound_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lp['CreateJob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['CreateJob'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation of the variable Createjob\n",
    "#imp = SimpleImputer(missing_values=0,strategy = 'mean')\n",
    "\n",
    "#mode_imp = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "#dfseries = lp.CreateJob\n",
    "\n",
    "#dfseries.values\n",
    "#dfseries.values.reshape(-1,1).shape\n",
    "#reshdf = dfseries.values.reshape(-1,1)\n",
    "\n",
    "#reshdf.shape\n",
    "#imp.fit_transform(reshdf)\n",
    "#lp.CreateJob = imp.fit_transform(reshdf)\n",
    "#lp['CreateJob'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az=pp.ProfileReport(lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation of the variable ChgOffDate\n",
    "#imp = SimpleImputer(missing_values=np.NaN ,strategy = 'most_frequent')\n",
    "\n",
    "#mode_imp = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "#dfseries = lp.ChgOffDate\n",
    "\n",
    "#dfseries.values\n",
    "#dfseries.values.reshape(-1,1).shape\n",
    "#reshdf = dfseries.values.reshape(-1,1)\n",
    "\n",
    "#reshdf.shape\n",
    "#imp.fit_transform(reshdf)\n",
    "#lp.ChgOffDate = imp.fit_transform(reshdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(lp.ChgOffDate).sum()\n",
    "#remove the ChgOffDate because, 80% of NA values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['ChgOffDate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['ChgOffDate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['NewExist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Exist variable having zeroes. using imputation replacing the zero value\n",
    "\n",
    "imp = SimpleImputer(missing_values=0,strategy = 'most_frequent')\n",
    "\n",
    "mode_imp = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "dfseries = lp.NewExist\n",
    "\n",
    "dfseries.values\n",
    "dfseries.values.reshape(-1,1).shape\n",
    "reshdf = dfseries.values.reshape(-1,1)\n",
    "\n",
    "reshdf.shape\n",
    "imp.fit_transform(reshdf)\n",
    "lp.NewExist = imp.fit_transform(reshdf)\n",
    "lp['NewExist'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp['NewExist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intuitively insignificant variables are deleted.\n",
    "lp.drop(['Name','ChgOffDate','City','BankState','Bank','State','CCSC','FranchiseCode','ChgOffPrinGr'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "# LabelEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_enc_dict={}\n",
    "lp_clean_enc=lp.copy()\n",
    "\n",
    "for col_name in lp_clean_enc:\n",
    "    \n",
    "    label_enc_dict[col_name]=LabelEncoder()\n",
    "    \n",
    "    col=lp_clean_enc[col_name]\n",
    "    col_not_null=col[col.notnull()]\n",
    "    reshaped_vals=col_not_null.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "    encoded_vals=label_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    lp_clean_enc.loc[col.notnull(),col_name]=np.squeeze(encoded_vals)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_clean_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_clean_enc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "#implementing RFECV\n",
    "X = lp_clean_enc.drop('MIS_Status', axis=1)\n",
    "target = lp_clean_enc['MIS_Status']\n",
    "rfc = RandomForestClassifier(random_state=101)\n",
    "rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "rfecv.fit(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal number of features: {}'.format(rfecv.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy based on number of features selected\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the features that are least important\n",
    "print(np.where(rfecv.support_ == False)[0])\n",
    "\n",
    "X.drop(X.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot showing the importance of the 1 features selected\n",
    "dset = pd.DataFrame()\n",
    "dset['attr'] = X.columns\n",
    "dset['importance'] = rfecv.estimator_.feature_importances_\n",
    "\n",
    "dset = dset.sort_values(by='importance', ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "plt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\n",
    "plt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.xlabel('Importance', fontsize=14, labelpad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_clean_enc['MIS_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='MIS_Status',data=lp,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling the Dependent Variable 'MIS_Status'\n",
    "# Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='MIS_Status',data=lp,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the chgoff and the pif dataset \n",
    "\n",
    "chgoff = lp_clean_enc[lp_clean_enc['MIS_Status']==1]\n",
    "\n",
    "pif = lp_clean_enc[lp_clean_enc['MIS_Status']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chgoff.shape,pif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling the data\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled = resample(pif,\n",
    "                        replace = False, # sample without replacement\n",
    "                        n_samples = len(chgoff), # match minority n\n",
    "                        random_state = 101) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chgoff.shape,pif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_downsampled = pd.concat([chgoff, downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_downsampled.MIS_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = lp_downsampled.MIS_Status\n",
    "X = lp_downsampled.drop('MIS_Status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='MIS_Status',data=lp_downsampled,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dowmsampled data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling of Resampling data(downsample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logreg.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(preds, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpreds = logreg.predict(xtest)\n",
    "\n",
    "accuracy_score(testpreds, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, testpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "\n",
    "model_dt = DecisionTreeClassifier(criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt = model_dt.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred_dt, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpred_dt = model_dt.predict(xtest)\n",
    "\n",
    "accuracy_score(testpred_dt, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, testpred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_jobs=3,oob_score=True,n_estimators=15,criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = model_rf.predict(xtrain)\n",
    "accuracy_score(pred_rf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpred_rf = model_rf.predict(xtest)\n",
    "accuracy_score(testpred_rf, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, testpred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling the Dependent Variable 'MIS_Status'\n",
    "# Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X,target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describes info about train and test set \n",
    "#print(\"Number transactions x_train dataset: \", x_train.shape) \n",
    "#print(\"Number transactions y_train dataset: \", y_train.shape) \n",
    "#print(\"Number transactions x_test dataset: \", x_test.shape) \n",
    "#print(\"Number transactions y_test dataset: \", y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "#print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "#from imblearn.over_sampling import SMOTE \n",
    "#sm = SMOTE(random_state = 2) \n",
    "#x_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel()) \n",
    "  \n",
    "#print('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape)) \n",
    "#print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "#print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "#print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check version number\n",
    "#import imblearn\n",
    "#print(imblearn.__version__)\n",
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn==0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling \n",
    "# upsampling sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Fraud and the normal dataset \n",
    "\n",
    "chgoff = lp_clean_enc[lp_clean_enc['MIS_Status']==1]\n",
    "\n",
    "pif = lp_clean_enc[lp_clean_enc['MIS_Status']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling the data\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = resample(chgoff,\n",
    "                        replace = True, # sample without replacement\n",
    "                         n_samples = len(pif), # match majority n\n",
    "                         random_state = 123) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chgoff.shape,pif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_upsampled = pd.concat([pif, upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_upsampled.MIS_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = lp_upsampled.MIS_Status\n",
    "X = lp_upsampled.drop('MIS_Status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='MIS_Status',data=lp_upsampled,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logreg.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(preds, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpreds = logreg.predict(xtest)\n",
    "\n",
    "accuracy_score(testpreds, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, testpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "\n",
    "model_dt = DecisionTreeClassifier(criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt = model_dt.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred_dt, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpred_dt = model_dt.predict(xtest)\n",
    "\n",
    "accuracy_score(testpred_dt, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, testpred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_jobs=3,oob_score=True,n_estimators=15,criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = model_rf.predict(xtrain)\n",
    "accuracy_score(pred_rf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpred_rf = model_rf.predict(xtest)\n",
    "accuracy_score(testpred_rf, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, testpred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training dataset in a dataframe using Pandas\n",
    "lp_test=pd.read_csv('../input/banking-sector-loan-prediction/test (1).csv')\n",
    "lp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(lp_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the values into 0 and 1 in RevLineCr\n",
    "lp_test['RevLineCr'].value_counts()\n",
    "lp_test['RevLineCr']=lp_test['RevLineCr'].map({'N':int('0'),'Y':int('1')})\n",
    "lp_test['RevLineCr'].value_counts()\n",
    "lp_test.info()\n",
    "RevLineCr_mode=lp_test['RevLineCr'].value_counts().index[0]\n",
    "RevLineCr_mode\n",
    "lp_test['RevLineCr'].fillna(RevLineCr_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='RevLineCr',data=lp_test,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the values into 0 and 1 in LowDoc\n",
    "lp_test['LowDoc'].value_counts()\n",
    "lp_test['LowDoc']=lp_test['LowDoc'].str.replace('C','Nan')\n",
    "lp_test['LowDoc']=lp_test['LowDoc'].str.replace('1','Nan')\n",
    "lp_test['LowDoc'].value_counts()\n",
    "lp_test['LowDoc']=lp_test['LowDoc'].map({'N':int('0'),'Y':int('1')})\n",
    "lp_test['LowDoc'].value_counts()\n",
    "lp_test.info()\n",
    "lp_test['LowDoc'].value_counts()\n",
    "LowDoc_mode=lp_test['LowDoc'].value_counts().index[0]\n",
    "LowDoc_mode\n",
    "lp_test['LowDoc'].fillna(LowDoc_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='LowDoc',data=lp_test,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in GrAppv\n",
    "lp_test['GrAppv'].value_counts()\n",
    "lp_test['GrAppv']=lp_test['GrAppv'].str.replace('$',' ')\n",
    "lp_test['GrAppv']=lp_test['GrAppv'].str.replace(',','')\n",
    "lp_test['GrAppv']=lp_test['GrAppv'].astype(float)\n",
    "lp_test['GrAppv'].value_counts()\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['GrAppv'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in SBA_Appv\n",
    "lp_test['SBA_Appv'].value_counts()\n",
    "lp_test['SBA_Appv']=lp_test['SBA_Appv'].str.replace('$',' ')\n",
    "lp_test['SBA_Appv']=lp_test['SBA_Appv'].str.replace(',','')\n",
    "lp_test['SBA_Appv']=lp_test['SBA_Appv'].astype(float)\n",
    "lp_test['SBA_Appv'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in SBA_Appv\n",
    "lp_test['ChgOffPrinGr'].value_counts()\n",
    "lp_test['ChgOffPrinGr']=lp_test['ChgOffPrinGr'].str.replace('$',' ')\n",
    "lp_test['ChgOffPrinGr']=lp_test['ChgOffPrinGr'].str.replace(',','')\n",
    "lp_test['ChgOffPrinGr']=lp_test['ChgOffPrinGr'].astype(float)\n",
    "lp_test['ChgOffPrinGr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in BalanceGross\n",
    "lp_test['BalanceGross'].value_counts()\n",
    "lp_test['BalanceGross']=lp_test['BalanceGross'].str.replace('$',' ')\n",
    "lp_test['BalanceGross']=lp_test['BalanceGross'].str.replace(',','')\n",
    "lp_test['BalanceGross']=lp_test['BalanceGross'].astype(float)\n",
    "lp_test['BalanceGross'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data \n",
    "#removing the string values like '$',',' in DisbursementGross\n",
    "lp_test['DisbursementGross'].value_counts()\n",
    "lp_test['DisbursementGross']=lp_test['DisbursementGross'].str.replace('$',' ')\n",
    "lp_test['DisbursementGross']=lp_test['DisbursementGross'].str.replace(',','')\n",
    "lp_test['DisbursementGross']=lp_test['DisbursementGross'].astype(float)\n",
    "lp_test['DisbursementGross'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DG_mode=lp_test['DisbursementDate'].value_counts().index[0]\n",
    "DG_mode\n",
    "lp_test['DisbursementDate'].fillna(DG_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['DisbursementGross'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "lp_test.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad=pp.ProfileReport(lp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = lp_test.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "lp_test.drop(lp_test[to_drop], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier of NoEmp\n",
    "#it is highly Skewed.removing the outliers and imputating it.\n",
    "\n",
    "## Perform all the steps of IQR\n",
    "sorted(lp_test['NoEmp'])\n",
    "quantile1, quantile3= np.percentile(lp_test['NoEmp'],[25,75])\n",
    "print(quantile1,quantile3)\n",
    "## Find the IQR\n",
    "iqr_value=quantile3-quantile1\n",
    "print(iqr_value)\n",
    "## Find the lower bound value and the higher bound value\n",
    "lower_bound_val = quantile1 -(1.5 * iqr_value) \n",
    "upper_bound_val = quantile3 +(1.5 * iqr_value)\n",
    "print(lower_bound_val,upper_bound_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['NoEmp'] = np.where(lp_test['NoEmp'] <-7.0, -7.0,lp_test['NoEmp'])\n",
    "lp_test['NoEmp'] = np.where(lp_test['NoEmp'] >17.0, 17.0,lp_test['NoEmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lp_test['NoEmp'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['NoEmp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['NoEmp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier of RetainedJob\n",
    "#it is highly Skewed.removing the outliers and imputating it.\n",
    "## Perform all the steps of IQR\n",
    "sorted(lp_test['RetainedJob'])\n",
    "quantile1, quantile3= np.percentile(lp_test['RetainedJob'],[25,75])\n",
    "print(quantile1,quantile3)\n",
    "## Find the IQR\n",
    "iqr_value=quantile3-quantile1\n",
    "print(iqr_value)\n",
    "## Find the lower bound value and the higher bound value\n",
    "lower_bound_val = quantile1 -(1.5 * iqr_value) \n",
    "upper_bound_val = quantile3 +(1.5 * iqr_value)\n",
    "print(lower_bound_val,upper_bound_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['RetainedJob'] = np.where(lp_test['RetainedJob'] <-6.0, -6.0,lp_test['RetainedJob'])\n",
    "lp_test['RetainedJob'] = np.where(lp_test['RetainedJob'] >10.0, 10.0,lp_test['RetainedJob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lp_test['RetainedJob'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['RetainedJob'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['RetainedJob'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RetainedJob variable having zeroes. using imputation replacing the zero value\n",
    "\n",
    "imp = SimpleImputer(missing_values=0,strategy = 'most_frequent')\n",
    "\n",
    "mode_imp = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "dfseries = lp_test.RetainedJob\n",
    "\n",
    "dfseries.values\n",
    "dfseries.values.reshape(-1,1).shape\n",
    "reshdf = dfseries.values.reshape(-1,1)\n",
    "\n",
    "reshdf.shape\n",
    "imp.fit_transform(reshdf)\n",
    "lp_test.RetainedJob = imp.fit_transform(reshdf)\n",
    "lp_test['RetainedJob'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lp_test['RetainedJob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier of RetainedJob\n",
    "#it is highly Skewed.removing the outliers and imputating it.\n",
    "## Perform all the steps of IQR\n",
    "sorted(lp_test['CreateJob'])\n",
    "quantile1, quantile3= np.percentile(lp_test['CreateJob'],[25,75])\n",
    "print(quantile1,quantile3)\n",
    "## Find the IQR\n",
    "iqr_value=quantile3-quantile1\n",
    "print(iqr_value)\n",
    "## Find the lower bound value and the higher bound value\n",
    "lower_bound_val = quantile1 -(1.5 * iqr_value) \n",
    "upper_bound_val = quantile3 +(1.5 * iqr_value)\n",
    "print(lower_bound_val,upper_bound_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lp_test['CreateJob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['CreateJob'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['NewExist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Exist variable having zeroes. using imputation replacing the zero value\n",
    "\n",
    "imp = SimpleImputer(missing_values=0,strategy = 'most_frequent')\n",
    "\n",
    "mode_imp = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "dfseries = lp_test.NewExist\n",
    "\n",
    "dfseries.values\n",
    "dfseries.values.reshape(-1,1).shape\n",
    "reshdf = dfseries.values.reshape(-1,1)\n",
    "\n",
    "reshdf.shape\n",
    "imp.fit_transform(reshdf)\n",
    "lp_test.NewExist = imp.fit_transform(reshdf)\n",
    "lp_test['NewExist'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test['NewExist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_mode=lp_test['Name'].value_counts().index[0]\n",
    "Name_mode\n",
    "lp_test['Name'].fillna(Name_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_mode=lp_test['State'].value_counts().index[0]\n",
    "State_mode\n",
    "lp_test['State'].fillna(State_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BankState_mode=lp_test['BankState'].value_counts().index[0]\n",
    "BankState_mode\n",
    "lp_test['BankState'].fillna(BankState_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_mode=lp_test['Bank'].value_counts().index[0]\n",
    "Bank_mode\n",
    "lp_test['Bank'].fillna(Bank_mode,inplace=True)\n",
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intuitively insignificant variables are deleted.\n",
    "lp_test.drop(['Unnamed: 0','Name','ChgOffDate','City','BankState','Bank','State','CCSC','FranchiseCode','ChgOffPrinGr'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_test_enc_dict={}\n",
    "lp_test_clean_enc=lp_test.copy()\n",
    "\n",
    "for col_name in lp_test_clean_enc:\n",
    "    \n",
    "    label_test_enc_dict[col_name]=LabelEncoder()\n",
    "    \n",
    "    col=lp_test_clean_enc[col_name]\n",
    "    col_not_null=col[col.notnull()]\n",
    "    reshaped_vals=col_not_null.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "    encoded_vals=label_test_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    lp_test_clean_enc.loc[col.notnull(),col_name]=np.squeeze(encoded_vals)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test_clean_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test_clean_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Save the trained model as a pickle string. \n",
    "with open('../working/model.pkl','wb') as fid:\n",
    "    pickle.dump(model_rf,fid)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
